{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Learning to Play SpaceInvaders using Tensorflow\n",
    "\n",
    "In this notebook I attempted to create an agent that plays the game space invaders using atari 2600.\n",
    "\n",
    "This notebook is inspired from the tutorial given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sourabh\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env\\lib\\site-packages\\IPython\\core\\display.py:689: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/gCJyVX98KJ4?showinfo=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/gCJyVX98KJ4?showinfo=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # Deep Learning Library\n",
    "import numpy as np # Handle Matrices\n",
    "import retro # Retro Environment\n",
    "\n",
    "from skimage import transform # helps to preprocess the frames\n",
    "from skimage.color import rgb2gray # Helps to convert frames from color to grayscale\n",
    "\n",
    "import matplotlib.pyplot as plt # Displays graphs\n",
    "\n",
    "from collections import deque # Ordered Collection with ends\n",
    "\n",
    "import random\n",
    "\n",
    "import warnings # This ignores all the warning messages that are normally printed during training because of skimage\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Environment\n",
    "\n",
    "This time we use **OpenAI Retro**, a wrapper for video game emulator cores using the Liberto API to turn them into gym environments\n",
    "\n",
    "### Our Environment\n",
    "\n",
    "We use the Atari Space Invaders environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of our frame is :  Box(210, 160, 3)\n",
      "The action size is :  8\n"
     ]
    }
   ],
   "source": [
    "# Creating the environment\n",
    "env = retro.make(game='SpaceInvaders-Atari2600')\n",
    "\n",
    "print(\"The size of our frame is : \",env.observation_space)\n",
    "print(\"The action size is : \",env.action_space.n)\n",
    "\n",
    "# Here we create an one-hot encoded version of our actions\n",
    "# possible actions = [[1, 0, 0, 0, 0, 0, 0, 0],[0, 1, 0, 0, 0, 0, 0, 0]...]\n",
    "possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Preprocessing functions\n",
    "\n",
    "Preprocessing is an important step, because we want to reduce the complexity of our states to reduce the computation time needed for training.\n",
    "\n",
    "Our Steps Taken :\n",
    "\n",
    "1. Grayscale each of the frames (Reason : Color doesn't add important information)\n",
    "2. Crop the screen (in our case we remove the part below the player, because it doesn't add any important information)\n",
    "3. We normalize the pixel values\n",
    "4. Finally we re-size the preprocessed frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame) :\n",
    "    \n",
    "    # Grayscale the frame\n",
    "    gray = rgb2gray(frame)\n",
    "    \n",
    "    # Crop the scree (remove the part below the player)\n",
    "    # [Up: Down, Left: Right]\n",
    "    cropped_frame = gray[0:-12,4:-12]\n",
    "    \n",
    "    # Normalize Pixel values\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "    \n",
    "    # Resize\n",
    "    preprocessed_frame = transform.resize(cropped_frame,[110,84])\n",
    "    \n",
    "    return preprocessed_frame # 110x84x1 frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stack_frames\n",
    "\n",
    "Stacking frames is really important because, it helps us to **give a sense of motion to our Neural Network.**\n",
    "\n",
    "But, We don't stack each frame, **we skip 4 frames at each time step**. This means that only every fourth frame is considered. And then, we use this frame to form the stack_frame\n",
    "\n",
    "**The frame skipping method is already implemented in the library**\n",
    "\n",
    "* First we preprocess the frame\n",
    "* Then we append the frame to the deque that automatically **removes the oldest frame**\n",
    "* Finally we build the stacked state\n",
    "\n",
    "How we work stack\n",
    "\n",
    "* For the first frame, we need 4 frames\n",
    "* At each timestep, **we add the new frame to deque and then we stack them to form a new stacked frame**\n",
    "* And so on\n",
    "* If we're done. **we create a new stack with 4 new frames (because we are in a new episode)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4 # We stack 4 frames\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames = deque([np.zeros((110,84),dtype=np.int) for i in range(stack_size)],maxlen=4)\n",
    "def stack_frames(stacked_frames,state,is_new_episode) :\n",
    "    \n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "    if is_new_episode :\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((110,84),dtype=np.int) for i in range(stack_size)],maxlen=4)\n",
    "        \n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        \n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames,axis=2)\n",
    "        \n",
    "    else :\n",
    "        # Append the frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames,axis=2)\n",
    "        \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the hyper-parameters\n",
    "\n",
    "In this part we'll set up our different hyperparameters. But when you implement a Neural Network **you will not implement hyperparameters at once but progressively**\n",
    "\n",
    "* First you begin by defining the neural networks hyperparameters when you implement the model\n",
    "\n",
    "* Then you'll add the training hyperparameters when you implement the training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Hyperparameters\n",
    "state_size = [110,84,4]          # Our input is a stack of 4 frames hence 110x84x4 (Width, height, channels)\n",
    "action_size = env.action_space.n # 8 possible actions\n",
    "learning_rate = 0.00025 # Alpha (aka learning rate)\n",
    "\n",
    "# Training Hyperparameters\n",
    "total_episodes = 5  # Total episodes for training\n",
    "max_steps = 500    # Max Possible steps in an episode\n",
    "batch_size = 64   # Batch Size\n",
    "\n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0        # exploration probability at start\n",
    "explore_stop = 0.01        # minimum exploration probability\n",
    "decay_rate = 0.00001      # exponential decay rate for exploration probability\n",
    "\n",
    "\n",
    "# Q-Learning hyperparameters\n",
    "gamma = 0.9          # Discounting Rate\n",
    "\n",
    "\n",
    "# Memory Hyperparameters\n",
    "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
    "\n",
    "memory_size = 1000000         # Number of experiences the Memory can keep\n",
    "\n",
    "# Preprocessing hyperparameters\n",
    "stack_size = 4       # Number of frames stacked\n",
    "\n",
    "# Modify if you want to see the trained agent\n",
    "training = True\n",
    "\n",
    "# Turn this to TRUE if you want to render the environment\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Deep Q-Learning Neural Network Mode\n",
    "\n",
    "This is our Deep Q-Learning Model :\n",
    "\n",
    "* We take a stack of 4 frames as input\n",
    "* It passes through 3 convnets\n",
    "* Then it is flatened\n",
    "* Finally it passes through 2 FC layers\n",
    "* It outputs a Q value for each actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork :\n",
    "    def __init__(self,state_size,action_size,learning_rate,name='DQNetwork') :\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(name) :\n",
    "            \n",
    "            # We create the placeholders\n",
    "            # *state_size means that we take each elements of state_size in tuple hence is like if we wrote \n",
    "            # [None,84,84,4]\n",
    "            self.inputs_ = tf.placeholder(tf.float32,[None,*state_size],name=\"inputs\")\n",
    "            self.actions_ = tf.placeholder(tf.float32,[None,self.action_size],name=\"actions_\")\n",
    "            \n",
    "            # Remember that target_Q is R(s,a) + gamma*max Q_hat(s',a')\n",
    "            self.target_Q = tf.placeholder(tf.float32,[None],name=\"target\")\n",
    "            \n",
    "            \n",
    "            '''\n",
    "            First Convnet :\n",
    "            CNN\n",
    "            ELU\n",
    "            '''\n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.inputs_,\n",
    "                                          filters=32,\n",
    "                                          kernel_size=[8,8],\n",
    "                                          strides=[4,4],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name=\"conv1\")\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1,name=\"conv1_out\")\n",
    "            \n",
    "            \n",
    "            '''\n",
    "            Second convnet :\n",
    "            CNN\n",
    "            ELU\n",
    "            '''\n",
    "            \n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.conv1_out,\n",
    "                                          filters=64,\n",
    "                                          kernel_size=[4,4],\n",
    "                                          strides=[2,2],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name=\"conv2\")\n",
    "            \n",
    "            self.conv2_out = tf.nn.elu(self.conv2,name=\"conv2_out\")\n",
    "            \n",
    "            \n",
    "            '''\n",
    "            Third Convnet :\n",
    "            CNN\n",
    "            ELU\n",
    "            '''\n",
    "            \n",
    "            self.conv3 = tf.layers.conv2d(inputs=self.conv2_out,\n",
    "                                          filters=64,\n",
    "                                          kernel_size=[3,3],\n",
    "                                          strides=[2,2],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name=\"conv3\")\n",
    "            \n",
    "            self.conv3_out = tf.nn.elu(self.conv3,name=\"conv3_out\")\n",
    "            \n",
    "            self.flatten = tf.contrib.layers.flatten(self.conv3_out)\n",
    "            \n",
    "            self.fc = tf.layers.dense(inputs=self.flatten,\n",
    "                                      units=512,\n",
    "                                      activation=tf.nn.elu,kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                      name=\"fc1\")\n",
    "            \n",
    "            self.output = tf.layers.dense(inputs=self.fc,\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          units=self.action_size,\n",
    "                                          activation=None)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Q is our predicted Q value.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output,self.actions_))\n",
    "            \n",
    "            \n",
    "            # The loss is the difference between our predicted Q_values and the Q_target\n",
    "            # Sum(Q_target - Q_predicted)^2\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Instantiate the DQNetwork\n",
    "DQNetwork = DQNetwork(state_size,action_size,learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay\n",
    "\n",
    "Now that we create our Neural Network, **we need to implement the Experience Replay Method**.\n",
    "\n",
    "Here we'll create the Memory Network object that creates a deque. A deque (*double ended queue*) is a data type that **removes the oldest element each time that you add a new element**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory() :\n",
    "    def __init__(self,max_size) :\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self,experience) :\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self,batch_size) :\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                 size=batch_size,\n",
    "                                 replace=False)\n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll **deal with the empty memory problem**. we pre-populate our memory by taking experience (state, action,reward,next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate memory\n",
    "memory = Memory(max_size=memory_size)\n",
    "for i in range(pretrain_length) :\n",
    "    \n",
    "    # If it's the first step\n",
    "    if i == 0 :\n",
    "        state = env.reset()\n",
    "        \n",
    "        state, stacked_frames = stack_frames(stacked_frames,state,True)\n",
    "        \n",
    "    # Get the next_state, the rewards, done by taking a random action\n",
    "    choice = random.randint(1,len(possible_actions)) - 1\n",
    "    action = possible_actions[choice]\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    \n",
    "    # Stack the frames\n",
    "    next_state, stacked_frames = stack_frames(stacked_frames,next_state,False)\n",
    "    \n",
    "    \n",
    "    # If the episode is finished (we're dead 3x)\n",
    "    if done :\n",
    "        \n",
    "        # We finished the episode\n",
    "        next_state =np.zeros(state.shape)\n",
    "        \n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state,action,reward,next_state,done))\n",
    "        \n",
    "        \n",
    "        # Start a new episode\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Stack the frames\n",
    "        state, stacked_frames = stack_frames(stacked_frames,state,True)\n",
    "    \n",
    "    else :\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state,action,reward,next_state,done))\n",
    "        \n",
    "        \n",
    "        # Our new state is now next_state\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Tensorboard\n",
    "\n",
    "To launch tensorboard : ```tensorboard --logdir=/tensorboard/dqn/1```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/dqn/1\")\n",
    "\n",
    "# Losses\n",
    "tf.summary.scalar(\"Loss\",DQNetwork.loss)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our agent\n",
    "\n",
    "Our Algorithm :\n",
    "\n",
    "* Initialize the weights\n",
    "* Init the environment\n",
    "* Initialize the decay rate (that will reduce epsilon)\n",
    "\n",
    "* **For** episode to max_episode **do**\n",
    "    1. Make a new episode\n",
    "    2. Set step to 0\n",
    "    3. Observe the first state s0\n",
    "    \n",
    "    4. **While** step < max_steps **do**\n",
    "        * Increase decay_rate\n",
    "        * With epsilon select a random actions a(t) otherwise select a(t) = argmax Q(s(t),a)\n",
    "        * Store transition S\n",
    "        * Sample random mini-batch from **D**\n",
    "        * Set Q_hat = r if the episode ends at +1 otherwise set Q_hat = r + gamma*max(Q(s',a'))\n",
    "        * Make a gradient descent step with loss (Q_hat - Q(s,a))^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function will do the part \n",
    "With epsilon select a random action a(t), otherwise select a(t) = argmax Q(s(t),a)\n",
    "'''\n",
    "\n",
    "def predict_action(explore_start,explore_stop,decay_rate,decay_step,state,actions) :\n",
    "    \n",
    "    # Epsilon Greedy Strategy\n",
    "    # Choose action a, from state s using epsilon greedy\n",
    "    # First we randomize a number\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "    \n",
    "    \n",
    "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook \n",
    "    explore_probability = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*decay_step)\n",
    "    \n",
    "    \n",
    "    if (explore_probability > exp_exp_tradeoff) :\n",
    "        # Make a random acton (exploration)\n",
    "        choice = random.randint(1,len(possible_actions)) - 1\n",
    "        action = possible_actions[choice]\n",
    "        \n",
    "    else :\n",
    "        # Get action from Q-network\n",
    "        # Estimate the Qs values state\n",
    "        Qs = sess.run(DQNetwork.output,feed_dict={DQNetwork.inputs_ : state.reshape((1,*state.shape))})\n",
    "        \n",
    "        # Take the biggest Q value (= best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[choice]\n",
    "        \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "# Saver will help us to save the model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training == True :\n",
    "    with tf.Session() as sess :\n",
    "        \n",
    "        # Initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Initialize the decay rate (that will use to reduce epsilon)\n",
    "        decay_step = 0\n",
    "        \n",
    "        for episode in range(total_episodes) :\n",
    "            \n",
    "            # Set step to 0\n",
    "            step = 0\n",
    "            \n",
    "            # Initialize the rewards of the episode\n",
    "            episode_rewards = []\n",
    "            \n",
    "            # Make a new episode and observe the first state\n",
    "            state = env.reset()\n",
    "            \n",
    "            # Remember that stack frame function also call our preprocess function.\n",
    "            state, stacked_frames = stack_frames(stacked_frames,state,True)\n",
    "            \n",
    "            while step < max_steps :\n",
    "                \n",
    "                step += 1\n",
    "                \n",
    "                # Increase decay_step\n",
    "                decay_step += 1\n",
    "                \n",
    "                # Predict the action to take and take it\n",
    "                action, explore_probability = predict_action(explore_start,explore_stop,decay_rate,decay_step,state,possible_actions)\n",
    "                \n",
    "                \n",
    "                # Perform the action and get the next state, reward, and done information \n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                \n",
    "                if episode_render :\n",
    "                    env.render()\n",
    "                    \n",
    "                # Add the reward to the total reward\n",
    "                episode_rewards.append(reward)\n",
    "                \n",
    "                # If the game is finished\n",
    "                if done :\n",
    "                    \n",
    "                    # The episode ends so no next state\n",
    "                    next_state = np.zeros((110,84),dtype=np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames,next_state,False)\n",
    "                    \n",
    "                    # Set step = max_steps to end the episode\n",
    "                    step = max_steps\n",
    "                    \n",
    "                    # Get the total reward of the episode\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "                    \n",
    "                    print('Episode: {}'.format(episode),'Total Reward : {}'.format(total_reward),'Explore P :{:.4f}'.format(explore_probability),'Training Loss {:4.f}'.format(loss))\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    rewards_list.append((episode,total_reward))\n",
    "                    \n",
    "                    # Store transition <s(t),a(t),r(t+1),s(t+1)> in memory D\n",
    "                    memory.add((state,action,reward,next_state,done))\n",
    "                    \n",
    "                else :\n",
    "                    \n",
    "                    # Stack the frame of the next_state\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames,next_state,False)\n",
    "                    \n",
    "                    # Add experience to memory\n",
    "                    memory.add((state,action,reward,next_state,done))\n",
    "                    \n",
    "                    # s(t+1) is now out current state\n",
    "                    state = next_state\n",
    "                    \n",
    "            # Learning Part\n",
    "            # Obtain random mini batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states_mb = np.array([each[0] for each in batch],ndmin=3)\n",
    "            actions_mb = np.array([each[1] for each in batch])\n",
    "            rewards_mb = np.array([each[2] for each in batch])\n",
    "            next_states_mb = np.array([each[3] for each in batch],ndmin=3)\n",
    "            dones_mb = np.array([each[4] for each in batch])\n",
    "            \n",
    "            target_Qs_batch = []\n",
    "            \n",
    "            \n",
    "            # Get Q values for next_state\n",
    "            Qs_next_state = sess.run(DQNetwork.output,feed_dict={DQNetwork.inputs_ : next_states_mb})\n",
    "            \n",
    "            \n",
    "            # Set Q_target = r if the episode at s+1, other wise set Q_target = r + gamma*max(Q(s',a'))\n",
    "            \n",
    "            for i in range(0,len(batch)) :\n",
    "                terminal = dones_mb[i]\n",
    "                \n",
    "                # If we are in terminal state, only equals reward\n",
    "                if terminal :\n",
    "                    target_Qs_batch.append(rewards_mb[i])\n",
    "                    \n",
    "                else :\n",
    "                    target = rewards_mb[i] + gamma*np.max(Qs_next_state[i])\n",
    "                    target_Qs_batch.append(target)\n",
    "                    \n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "                \n",
    "                loss, _ = sess.run([DQNetwork.loss,DQNetwork.optimizer],feed_dict={DQNetwork.inputs_:states_mb,\n",
    "                                                                                   DQNetwork.target_Q:targets_mb,\n",
    "                                                                                   DQNetwork.actions_:actions_mb})\n",
    "                \n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op,feed_dict={DQNetwork.inputs_:states_mb,\n",
    "                                                       DQNetwork.target_Q : targets_mb,\n",
    "                                                       DQNetwork.actions_:actions_mb})\n",
    "                writer.add_summary(summary,episode)\n",
    "                writer.flush()\n",
    "                \n",
    "            # Save model every 5 episodes\n",
    "            if episode % 5 == 0 :\n",
    "                save_path = saver.save(sess,\"./models/model.ckpt\")\n",
    "                print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and Watching our agent play\n",
    "\n",
    "Now that we trained our agent, we can test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "^^^^^^^^^^^^^^^^^^^^^\n",
      "EPISODE 0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess :\n",
    "    total_test_rewards = []\n",
    "    \n",
    "    # load saved model\n",
    "    saver.restore(sess,\"./models/model.ckpt\")\n",
    "    for episode in range(1) :\n",
    "        total_rewards = 0\n",
    "        \n",
    "        state = env.reset()\n",
    "        state, stacked_frames = stack_frames(stacked_frames,state,True)\n",
    "        \n",
    "        print('^^^^^^^^^^^^^^^^^^^^^')\n",
    "        print('EPISODE',episode)\n",
    "        \n",
    "        while True :\n",
    "            \n",
    "            # Reshape the state\n",
    "            state = state.reshape((1,*state_size))\n",
    "            \n",
    "            # Get action from Q-network\n",
    "            # Estimate the Qs values state\n",
    "            Qs = sess.run(DQNetwork.output,feed_dict={DQNetwork.inputs_:state})\n",
    "            \n",
    "            \n",
    "            # Take the biggest Q value (= the best action)\n",
    "            choice = np.argmax(Qs)\n",
    "            action = possible_actions[choice]\n",
    "            \n",
    "            # Perform the action and get the next_state, reward, and done information\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            env.render()\n",
    "            \n",
    "            total_rewards += reward\n",
    "            \n",
    "            if done :\n",
    "                print('Score',total_rewards)\n",
    "                total_test_rewards.append(total_rewards)\n",
    "                break\n",
    "                \n",
    "            next_state, stacked_frames = stack_frames(stacked_frames,next_state,False)\n",
    "            state = next_state\n",
    "        \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
