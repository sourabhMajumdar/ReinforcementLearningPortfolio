{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Create our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env = env.unwrapped\n",
    "# Policy gradient has high variance, seed for reproducibility\n",
    "env.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Setting up hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Environment Hyperparameters\n",
    "state_size = 4\n",
    "action_size = env.action_space.n\n",
    "\n",
    "## Training Hyperparameters\n",
    "max_episodes = 300\n",
    "learning_rate = 0.01\n",
    "gamma = 0.95 # Discount Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Define the preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(episode_rewards) :\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    \n",
    "    for i in reversed(range(len(episode_rewards))) :\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "        \n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
    "    \n",
    "    return discounted_episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 : Create our Policy Gradient Neural Network\n",
    "\n",
    "The idea is simple :\n",
    "\n",
    "* Our state which is an array of 4 values will be used as an input\n",
    "* Our NN is 3 fully connected layers\n",
    "* Our outpus activation function is softmax that squashes the outputs to a probability distribution (for instance if we have 4,2,6 --> softmax --> (0.4,0.2,0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"inputs\") :\n",
    "    input_ = tf.placeholder(tf.float32,[None,state_size],name=\"input_\")\n",
    "    actions = tf.placeholder(tf.int32,[None,action_size],name=\"actions\")\n",
    "    discounted_episode_rewards_ = tf.placeholder(tf.float32,[None],name=\"discounted_episode_rewards\")\n",
    "    \n",
    "    # Add this placeholder for having this variable in tensorboard\n",
    "    mean_reward_ = tf.placeholder(tf.float32,name=\"mean_reward\")\n",
    "    \n",
    "    with tf.name_scope(\"fc1\") : \n",
    "        fc1 = tf.contrib.layers.fully_connected(inputs=input_,\n",
    "                                                num_outputs=10,\n",
    "                                                activation_fn=tf.nn.relu,\n",
    "                                                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    with tf.name_scope(\"fc2\") :\n",
    "        fc2 = tf.contrib.layers.fully_connected(inputs=fc1,\n",
    "                                                num_outputs=action_size,\n",
    "                                                activation_fn=tf.nn.relu,\n",
    "                                                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    with tf.name_scope(\"fc3\") :\n",
    "        fc3 = tf.contrib.layers.fully_connected(inputs=fc2,\n",
    "                                                num_outputs=action_size,\n",
    "                                                activation_fn=None,\n",
    "                                                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "    with tf.name_scope(\"softmax\") :\n",
    "        action_distribution = tf.nn.softmax(fc3)\n",
    "        \n",
    "    with tf.name_scope(\"loss\") :\n",
    "        # tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function\n",
    "        # If you have single-class labels, where an object can only belong to one class, you might can consider using\n",
    "        # tf.nn.sparse_softmax_cross_entropy_with_logits so that you don't have to convert your labels to a dense one-hot array.\n",
    "        \n",
    "        neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=fc3,labels=actions)\n",
    "        loss = tf.reduce_mean(neg_log_prob*discounted_episode_rewards_)\n",
    "        \n",
    "    with tf.name_scope(\"train\") :\n",
    "        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 : Setting up Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/pg/1\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\",loss)\n",
    "\n",
    "## Reward mean\n",
    "tf.summary.scalar(\"Reward_mean\",mean_reward_)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 : Train our Agent\n",
    "\n",
    "Create the NN maxReward = 0 # Keep track of maximum reward For episode in range(max_episodes) : episode + 1 reset environmet, reset stores (states,actions,rewards) For each step : Choose action a Perform action a Store s, a, r If done : Calculate Sum reward Calculate gamma Gt Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================\n",
      "Episode:  0\n",
      "Reward:  19.0\n",
      "Mean Reward  19.0\n",
      "Max reward so far:  19.0\n",
      "=======================================\n",
      "Episode:  1\n",
      "Reward:  58.0\n",
      "Mean Reward  38.5\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  2\n",
      "Reward:  23.0\n",
      "Mean Reward  33.333333333333336\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  3\n",
      "Reward:  13.0\n",
      "Mean Reward  28.25\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  4\n",
      "Reward:  29.0\n",
      "Mean Reward  28.4\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  5\n",
      "Reward:  41.0\n",
      "Mean Reward  30.5\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  6\n",
      "Reward:  24.0\n",
      "Mean Reward  29.571428571428573\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  7\n",
      "Reward:  16.0\n",
      "Mean Reward  27.875\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  8\n",
      "Reward:  18.0\n",
      "Mean Reward  26.77777777777778\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  9\n",
      "Reward:  42.0\n",
      "Mean Reward  28.3\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  10\n",
      "Reward:  13.0\n",
      "Mean Reward  26.90909090909091\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  11\n",
      "Reward:  24.0\n",
      "Mean Reward  26.666666666666668\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  12\n",
      "Reward:  15.0\n",
      "Mean Reward  25.76923076923077\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  13\n",
      "Reward:  23.0\n",
      "Mean Reward  25.571428571428573\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  14\n",
      "Reward:  19.0\n",
      "Mean Reward  25.133333333333333\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  15\n",
      "Reward:  34.0\n",
      "Mean Reward  25.6875\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  16\n",
      "Reward:  16.0\n",
      "Mean Reward  25.11764705882353\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  17\n",
      "Reward:  18.0\n",
      "Mean Reward  24.72222222222222\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  18\n",
      "Reward:  18.0\n",
      "Mean Reward  24.36842105263158\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  19\n",
      "Reward:  25.0\n",
      "Mean Reward  24.4\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  20\n",
      "Reward:  14.0\n",
      "Mean Reward  23.904761904761905\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  21\n",
      "Reward:  14.0\n",
      "Mean Reward  23.454545454545453\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  22\n",
      "Reward:  13.0\n",
      "Mean Reward  23.0\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  23\n",
      "Reward:  14.0\n",
      "Mean Reward  22.625\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  24\n",
      "Reward:  16.0\n",
      "Mean Reward  22.36\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  25\n",
      "Reward:  18.0\n",
      "Mean Reward  22.192307692307693\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  26\n",
      "Reward:  23.0\n",
      "Mean Reward  22.22222222222222\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  27\n",
      "Reward:  23.0\n",
      "Mean Reward  22.25\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  28\n",
      "Reward:  30.0\n",
      "Mean Reward  22.517241379310345\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  29\n",
      "Reward:  28.0\n",
      "Mean Reward  22.7\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  30\n",
      "Reward:  33.0\n",
      "Mean Reward  23.032258064516128\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  31\n",
      "Reward:  19.0\n",
      "Mean Reward  22.90625\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  32\n",
      "Reward:  22.0\n",
      "Mean Reward  22.87878787878788\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  33\n",
      "Reward:  30.0\n",
      "Mean Reward  23.08823529411765\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  34\n",
      "Reward:  17.0\n",
      "Mean Reward  22.914285714285715\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  35\n",
      "Reward:  16.0\n",
      "Mean Reward  22.72222222222222\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  36\n",
      "Reward:  14.0\n",
      "Mean Reward  22.486486486486488\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  37\n",
      "Reward:  16.0\n",
      "Mean Reward  22.31578947368421\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  38\n",
      "Reward:  21.0\n",
      "Mean Reward  22.28205128205128\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  39\n",
      "Reward:  11.0\n",
      "Mean Reward  22.0\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  40\n",
      "Reward:  12.0\n",
      "Mean Reward  21.75609756097561\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  41\n",
      "Reward:  9.0\n",
      "Mean Reward  21.452380952380953\n",
      "Max reward so far:  58.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  42\n",
      "Reward:  80.0\n",
      "Mean Reward  22.813953488372093\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  43\n",
      "Reward:  16.0\n",
      "Mean Reward  22.65909090909091\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  44\n",
      "Reward:  17.0\n",
      "Mean Reward  22.533333333333335\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  45\n",
      "Reward:  17.0\n",
      "Mean Reward  22.41304347826087\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  46\n",
      "Reward:  12.0\n",
      "Mean Reward  22.19148936170213\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  47\n",
      "Reward:  23.0\n",
      "Mean Reward  22.208333333333332\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  48\n",
      "Reward:  14.0\n",
      "Mean Reward  22.040816326530614\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  49\n",
      "Reward:  11.0\n",
      "Mean Reward  21.82\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  50\n",
      "Reward:  18.0\n",
      "Mean Reward  21.745098039215687\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  51\n",
      "Reward:  18.0\n",
      "Mean Reward  21.673076923076923\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  52\n",
      "Reward:  25.0\n",
      "Mean Reward  21.735849056603772\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  53\n",
      "Reward:  23.0\n",
      "Mean Reward  21.75925925925926\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  54\n",
      "Reward:  20.0\n",
      "Mean Reward  21.727272727272727\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  55\n",
      "Reward:  22.0\n",
      "Mean Reward  21.732142857142858\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  56\n",
      "Reward:  10.0\n",
      "Mean Reward  21.526315789473685\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  57\n",
      "Reward:  20.0\n",
      "Mean Reward  21.5\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  58\n",
      "Reward:  9.0\n",
      "Mean Reward  21.28813559322034\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  59\n",
      "Reward:  14.0\n",
      "Mean Reward  21.166666666666668\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  60\n",
      "Reward:  20.0\n",
      "Mean Reward  21.147540983606557\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  61\n",
      "Reward:  12.0\n",
      "Mean Reward  21.0\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  62\n",
      "Reward:  18.0\n",
      "Mean Reward  20.952380952380953\n",
      "Max reward so far:  80.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "=======================================\n",
      "Episode:  63\n",
      "Reward:  10.0\n",
      "Mean Reward  20.78125\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  64\n",
      "Reward:  43.0\n",
      "Mean Reward  21.123076923076923\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  65\n",
      "Reward:  25.0\n",
      "Mean Reward  21.181818181818183\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  66\n",
      "Reward:  20.0\n",
      "Mean Reward  21.16417910447761\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  67\n",
      "Reward:  43.0\n",
      "Mean Reward  21.485294117647058\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  68\n",
      "Reward:  21.0\n",
      "Mean Reward  21.47826086956522\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  69\n",
      "Reward:  18.0\n",
      "Mean Reward  21.428571428571427\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  70\n",
      "Reward:  20.0\n",
      "Mean Reward  21.408450704225352\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  71\n",
      "Reward:  42.0\n",
      "Mean Reward  21.694444444444443\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  72\n",
      "Reward:  14.0\n",
      "Mean Reward  21.589041095890412\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  73\n",
      "Reward:  18.0\n",
      "Mean Reward  21.54054054054054\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  74\n",
      "Reward:  18.0\n",
      "Mean Reward  21.493333333333332\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  75\n",
      "Reward:  24.0\n",
      "Mean Reward  21.526315789473685\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  76\n",
      "Reward:  51.0\n",
      "Mean Reward  21.90909090909091\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  77\n",
      "Reward:  26.0\n",
      "Mean Reward  21.96153846153846\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  78\n",
      "Reward:  8.0\n",
      "Mean Reward  21.78481012658228\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  79\n",
      "Reward:  19.0\n",
      "Mean Reward  21.75\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  80\n",
      "Reward:  11.0\n",
      "Mean Reward  21.617283950617285\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  81\n",
      "Reward:  15.0\n",
      "Mean Reward  21.536585365853657\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  82\n",
      "Reward:  53.0\n",
      "Mean Reward  21.91566265060241\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  83\n",
      "Reward:  20.0\n",
      "Mean Reward  21.892857142857142\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  84\n",
      "Reward:  14.0\n",
      "Mean Reward  21.8\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  85\n",
      "Reward:  55.0\n",
      "Mean Reward  22.186046511627907\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  86\n",
      "Reward:  14.0\n",
      "Mean Reward  22.091954022988507\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  87\n",
      "Reward:  10.0\n",
      "Mean Reward  21.954545454545453\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  88\n",
      "Reward:  19.0\n",
      "Mean Reward  21.921348314606742\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  89\n",
      "Reward:  25.0\n",
      "Mean Reward  21.955555555555556\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  90\n",
      "Reward:  13.0\n",
      "Mean Reward  21.857142857142858\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  91\n",
      "Reward:  39.0\n",
      "Mean Reward  22.043478260869566\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  92\n",
      "Reward:  16.0\n",
      "Mean Reward  21.978494623655912\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  93\n",
      "Reward:  13.0\n",
      "Mean Reward  21.882978723404257\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  94\n",
      "Reward:  12.0\n",
      "Mean Reward  21.778947368421054\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  95\n",
      "Reward:  15.0\n",
      "Mean Reward  21.708333333333332\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  96\n",
      "Reward:  16.0\n",
      "Mean Reward  21.649484536082475\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  97\n",
      "Reward:  26.0\n",
      "Mean Reward  21.693877551020407\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  98\n",
      "Reward:  33.0\n",
      "Mean Reward  21.80808080808081\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  99\n",
      "Reward:  26.0\n",
      "Mean Reward  21.85\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  100\n",
      "Reward:  29.0\n",
      "Mean Reward  21.92079207920792\n",
      "Max reward so far:  80.0\n",
      "=======================================\n",
      "Episode:  101\n",
      "Reward:  22.0\n",
      "Mean Reward  21.92156862745098\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  102\n",
      "Reward:  41.0\n",
      "Mean Reward  22.106796116504853\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  103\n",
      "Reward:  26.0\n",
      "Mean Reward  22.14423076923077\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  104\n",
      "Reward:  12.0\n",
      "Mean Reward  22.047619047619047\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  105\n",
      "Reward:  32.0\n",
      "Mean Reward  22.141509433962263\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  106\n",
      "Reward:  43.0\n",
      "Mean Reward  22.33644859813084\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  107\n",
      "Reward:  21.0\n",
      "Mean Reward  22.324074074074073\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  108\n",
      "Reward:  60.0\n",
      "Mean Reward  22.6697247706422\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  109\n",
      "Reward:  23.0\n",
      "Mean Reward  22.672727272727272\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  110\n",
      "Reward:  44.0\n",
      "Mean Reward  22.864864864864863\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  111\n",
      "Reward:  28.0\n",
      "Mean Reward  22.910714285714285\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  112\n",
      "Reward:  10.0\n",
      "Mean Reward  22.79646017699115\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  113\n",
      "Reward:  17.0\n",
      "Mean Reward  22.74561403508772\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  114\n",
      "Reward:  25.0\n",
      "Mean Reward  22.765217391304347\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  115\n",
      "Reward:  10.0\n",
      "Mean Reward  22.655172413793103\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  116\n",
      "Reward:  18.0\n",
      "Mean Reward  22.615384615384617\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  117\n",
      "Reward:  45.0\n",
      "Mean Reward  22.805084745762713\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  118\n",
      "Reward:  35.0\n",
      "Mean Reward  22.907563025210084\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  119\n",
      "Reward:  58.0\n",
      "Mean Reward  23.2\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  120\n",
      "Reward:  41.0\n",
      "Mean Reward  23.34710743801653\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  121\n",
      "Reward:  44.0\n",
      "Mean Reward  23.516393442622952\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  122\n",
      "Reward:  33.0\n",
      "Mean Reward  23.59349593495935\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  123\n",
      "Reward:  43.0\n",
      "Mean Reward  23.75\n",
      "Max reward so far:  80.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "=======================================\n",
      "Episode:  124\n",
      "Reward:  35.0\n",
      "Mean Reward  23.84\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  125\n",
      "Reward:  33.0\n",
      "Mean Reward  23.91269841269841\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  126\n",
      "Reward:  27.0\n",
      "Mean Reward  23.937007874015748\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  127\n",
      "Reward:  15.0\n",
      "Mean Reward  23.8671875\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  128\n",
      "Reward:  52.0\n",
      "Mean Reward  24.085271317829456\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  129\n",
      "Reward:  17.0\n",
      "Mean Reward  24.03076923076923\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  130\n",
      "Reward:  11.0\n",
      "Mean Reward  23.931297709923665\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  131\n",
      "Reward:  46.0\n",
      "Mean Reward  24.098484848484848\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  132\n",
      "Reward:  16.0\n",
      "Mean Reward  24.037593984962406\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  133\n",
      "Reward:  68.0\n",
      "Mean Reward  24.365671641791046\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  134\n",
      "Reward:  20.0\n",
      "Mean Reward  24.333333333333332\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  135\n",
      "Reward:  24.0\n",
      "Mean Reward  24.330882352941178\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  136\n",
      "Reward:  32.0\n",
      "Mean Reward  24.386861313868614\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  137\n",
      "Reward:  22.0\n",
      "Mean Reward  24.369565217391305\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  138\n",
      "Reward:  32.0\n",
      "Mean Reward  24.424460431654676\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  139\n",
      "Reward:  40.0\n",
      "Mean Reward  24.535714285714285\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  140\n",
      "Reward:  25.0\n",
      "Mean Reward  24.53900709219858\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  141\n",
      "Reward:  41.0\n",
      "Mean Reward  24.654929577464788\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  142\n",
      "Reward:  16.0\n",
      "Mean Reward  24.594405594405593\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  143\n",
      "Reward:  19.0\n",
      "Mean Reward  24.555555555555557\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  144\n",
      "Reward:  28.0\n",
      "Mean Reward  24.579310344827586\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  145\n",
      "Reward:  19.0\n",
      "Mean Reward  24.541095890410958\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  146\n",
      "Reward:  21.0\n",
      "Mean Reward  24.517006802721088\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  147\n",
      "Reward:  28.0\n",
      "Mean Reward  24.54054054054054\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  148\n",
      "Reward:  49.0\n",
      "Mean Reward  24.70469798657718\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  149\n",
      "Reward:  12.0\n",
      "Mean Reward  24.62\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  150\n",
      "Reward:  38.0\n",
      "Mean Reward  24.70860927152318\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  151\n",
      "Reward:  19.0\n",
      "Mean Reward  24.67105263157895\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  152\n",
      "Reward:  54.0\n",
      "Mean Reward  24.862745098039216\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  153\n",
      "Reward:  47.0\n",
      "Mean Reward  25.006493506493506\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  154\n",
      "Reward:  59.0\n",
      "Mean Reward  25.225806451612904\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  155\n",
      "Reward:  15.0\n",
      "Mean Reward  25.16025641025641\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  156\n",
      "Reward:  45.0\n",
      "Mean Reward  25.286624203821656\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  157\n",
      "Reward:  14.0\n",
      "Mean Reward  25.21518987341772\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  158\n",
      "Reward:  67.0\n",
      "Mean Reward  25.47798742138365\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  159\n",
      "Reward:  64.0\n",
      "Mean Reward  25.71875\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  160\n",
      "Reward:  28.0\n",
      "Mean Reward  25.732919254658384\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  161\n",
      "Reward:  35.0\n",
      "Mean Reward  25.790123456790123\n",
      "Max reward so far:  80.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  162\n",
      "Reward:  90.0\n",
      "Mean Reward  26.1840490797546\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  163\n",
      "Reward:  16.0\n",
      "Mean Reward  26.121951219512194\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  164\n",
      "Reward:  36.0\n",
      "Mean Reward  26.181818181818183\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  165\n",
      "Reward:  57.0\n",
      "Mean Reward  26.367469879518072\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  166\n",
      "Reward:  46.0\n",
      "Mean Reward  26.48502994011976\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  167\n",
      "Reward:  21.0\n",
      "Mean Reward  26.452380952380953\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  168\n",
      "Reward:  26.0\n",
      "Mean Reward  26.449704142011836\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  169\n",
      "Reward:  15.0\n",
      "Mean Reward  26.38235294117647\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  170\n",
      "Reward:  16.0\n",
      "Mean Reward  26.321637426900583\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  171\n",
      "Reward:  14.0\n",
      "Mean Reward  26.25\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  172\n",
      "Reward:  16.0\n",
      "Mean Reward  26.190751445086704\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  173\n",
      "Reward:  37.0\n",
      "Mean Reward  26.25287356321839\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  174\n",
      "Reward:  12.0\n",
      "Mean Reward  26.17142857142857\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  175\n",
      "Reward:  75.0\n",
      "Mean Reward  26.448863636363637\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  176\n",
      "Reward:  22.0\n",
      "Mean Reward  26.423728813559322\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  177\n",
      "Reward:  27.0\n",
      "Mean Reward  26.426966292134832\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  178\n",
      "Reward:  11.0\n",
      "Mean Reward  26.34078212290503\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  179\n",
      "Reward:  20.0\n",
      "Mean Reward  26.305555555555557\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  180\n",
      "Reward:  22.0\n",
      "Mean Reward  26.281767955801104\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  181\n",
      "Reward:  25.0\n",
      "Mean Reward  26.274725274725274\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  182\n",
      "Reward:  17.0\n",
      "Mean Reward  26.224043715846996\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  183\n",
      "Reward:  18.0\n",
      "Mean Reward  26.179347826086957\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  184\n",
      "Reward:  39.0\n",
      "Mean Reward  26.248648648648647\n",
      "Max reward so far:  90.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "=======================================\n",
      "Episode:  185\n",
      "Reward:  45.0\n",
      "Mean Reward  26.349462365591396\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  186\n",
      "Reward:  18.0\n",
      "Mean Reward  26.3048128342246\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  187\n",
      "Reward:  39.0\n",
      "Mean Reward  26.372340425531913\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  188\n",
      "Reward:  42.0\n",
      "Mean Reward  26.455026455026456\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  189\n",
      "Reward:  25.0\n",
      "Mean Reward  26.44736842105263\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  190\n",
      "Reward:  16.0\n",
      "Mean Reward  26.392670157068064\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  191\n",
      "Reward:  41.0\n",
      "Mean Reward  26.46875\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  192\n",
      "Reward:  15.0\n",
      "Mean Reward  26.409326424870468\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  193\n",
      "Reward:  35.0\n",
      "Mean Reward  26.45360824742268\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  194\n",
      "Reward:  14.0\n",
      "Mean Reward  26.38974358974359\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  195\n",
      "Reward:  24.0\n",
      "Mean Reward  26.377551020408163\n",
      "Max reward so far:  90.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  196\n",
      "Reward:  96.0\n",
      "Mean Reward  26.730964467005077\n",
      "Max reward so far:  96.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  197\n",
      "Reward:  18.0\n",
      "Mean Reward  26.68686868686869\n",
      "Max reward so far:  96.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  198\n",
      "Reward:  11.0\n",
      "Mean Reward  26.608040201005025\n",
      "Max reward so far:  96.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  199\n",
      "Reward:  29.0\n",
      "Mean Reward  26.62\n",
      "Max reward so far:  96.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  200\n",
      "Reward:  87.0\n",
      "Mean Reward  26.920398009950247\n",
      "Max reward so far:  96.0\n",
      "=======================================\n",
      "Episode:  201\n",
      "Reward:  77.0\n",
      "Mean Reward  27.168316831683168\n",
      "Max reward so far:  96.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  202\n",
      "Reward:  124.0\n",
      "Mean Reward  27.645320197044335\n",
      "Max reward so far:  124.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  203\n",
      "Reward:  17.0\n",
      "Mean Reward  27.59313725490196\n",
      "Max reward so far:  124.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  204\n",
      "Reward:  126.0\n",
      "Mean Reward  28.073170731707318\n",
      "Max reward so far:  126.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  205\n",
      "Reward:  135.0\n",
      "Mean Reward  28.59223300970874\n",
      "Max reward so far:  135.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  206\n",
      "Reward:  15.0\n",
      "Mean Reward  28.52657004830918\n",
      "Max reward so far:  135.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  207\n",
      "Reward:  85.0\n",
      "Mean Reward  28.798076923076923\n",
      "Max reward so far:  135.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  208\n",
      "Reward:  63.0\n",
      "Mean Reward  28.961722488038276\n",
      "Max reward so far:  135.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  209\n",
      "Reward:  31.0\n",
      "Mean Reward  28.97142857142857\n",
      "Max reward so far:  135.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  210\n",
      "Reward:  36.0\n",
      "Mean Reward  29.00473933649289\n",
      "Max reward so far:  135.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  211\n",
      "Reward:  21.0\n",
      "Mean Reward  28.96698113207547\n",
      "Max reward so far:  135.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  212\n",
      "Reward:  48.0\n",
      "Mean Reward  29.056338028169016\n",
      "Max reward so far:  135.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  213\n",
      "Reward:  74.0\n",
      "Mean Reward  29.266355140186917\n",
      "Max reward so far:  135.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  214\n",
      "Reward:  51.0\n",
      "Mean Reward  29.367441860465117\n",
      "Max reward so far:  135.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  215\n",
      "Reward:  120.0\n",
      "Mean Reward  29.787037037037038\n",
      "Max reward so far:  135.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  216\n",
      "Reward:  37.0\n",
      "Mean Reward  29.820276497695854\n",
      "Max reward so far:  135.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  217\n",
      "Reward:  118.0\n",
      "Mean Reward  30.224770642201836\n",
      "Max reward so far:  135.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  218\n",
      "Reward:  16.0\n",
      "Mean Reward  30.159817351598175\n",
      "Max reward so far:  135.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  219\n",
      "Reward:  24.0\n",
      "Mean Reward  30.131818181818183\n",
      "Max reward so far:  135.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  220\n",
      "Reward:  103.0\n",
      "Mean Reward  30.46153846153846\n",
      "Max reward so far:  135.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  221\n",
      "Reward:  92.0\n",
      "Mean Reward  30.73873873873874\n",
      "Max reward so far:  135.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  222\n",
      "Reward:  27.0\n",
      "Mean Reward  30.721973094170405\n",
      "Max reward so far:  135.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  223\n",
      "Reward:  184.0\n",
      "Mean Reward  31.40625\n",
      "Max reward so far:  184.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  224\n",
      "Reward:  67.0\n",
      "Mean Reward  31.564444444444444\n",
      "Max reward so far:  184.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  225\n",
      "Reward:  141.0\n",
      "Mean Reward  32.04867256637168\n",
      "Max reward so far:  184.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  226\n",
      "Reward:  81.0\n",
      "Mean Reward  32.26431718061674\n",
      "Max reward so far:  184.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  227\n",
      "Reward:  23.0\n",
      "Mean Reward  32.223684210526315\n",
      "Max reward so far:  184.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  228\n",
      "Reward:  170.0\n",
      "Mean Reward  32.82532751091703\n",
      "Max reward so far:  184.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  229\n",
      "Reward:  73.0\n",
      "Mean Reward  33.0\n",
      "Max reward so far:  184.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  230\n",
      "Reward:  55.0\n",
      "Mean Reward  33.095238095238095\n",
      "Max reward so far:  184.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  231\n",
      "Reward:  117.0\n",
      "Mean Reward  33.456896551724135\n",
      "Max reward so far:  184.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  232\n",
      "Reward:  129.0\n",
      "Mean Reward  33.86695278969957\n",
      "Max reward so far:  184.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  233\n",
      "Reward:  26.0\n",
      "Mean Reward  33.833333333333336\n",
      "Max reward so far:  184.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  234\n",
      "Reward:  125.0\n",
      "Mean Reward  34.22127659574468\n",
      "Max reward so far:  184.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  235\n",
      "Reward:  17.0\n",
      "Mean Reward  34.148305084745765\n",
      "Max reward so far:  184.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  236\n",
      "Reward:  191.0\n",
      "Mean Reward  34.81012658227848\n",
      "Max reward so far:  191.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  237\n",
      "Reward:  217.0\n",
      "Mean Reward  35.575630252100844\n",
      "Max reward so far:  217.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  238\n",
      "Reward:  88.0\n",
      "Mean Reward  35.79497907949791\n",
      "Max reward so far:  217.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  239\n",
      "Reward:  50.0\n",
      "Mean Reward  35.854166666666664\n",
      "Max reward so far:  217.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  240\n",
      "Reward:  129.0\n",
      "Mean Reward  36.24066390041494\n",
      "Max reward so far:  217.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  241\n",
      "Reward:  122.0\n",
      "Mean Reward  36.59504132231405\n",
      "Max reward so far:  217.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  242\n",
      "Reward:  28.0\n",
      "Mean Reward  36.559670781893004\n",
      "Max reward so far:  217.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  243\n",
      "Reward:  131.0\n",
      "Mean Reward  36.94672131147541\n",
      "Max reward so far:  217.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  244\n",
      "Reward:  119.0\n",
      "Mean Reward  37.28163265306122\n",
      "Max reward so far:  217.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "=======================================\n",
      "Episode:  245\n",
      "Reward:  136.0\n",
      "Mean Reward  37.68292682926829\n",
      "Max reward so far:  217.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  246\n",
      "Reward:  164.0\n",
      "Mean Reward  38.19433198380567\n",
      "Max reward so far:  217.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  247\n",
      "Reward:  183.0\n",
      "Mean Reward  38.778225806451616\n",
      "Max reward so far:  217.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  248\n",
      "Reward:  24.0\n",
      "Mean Reward  38.71887550200803\n",
      "Max reward so far:  217.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  249\n",
      "Reward:  110.0\n",
      "Mean Reward  39.004\n",
      "Max reward so far:  217.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  250\n",
      "Reward:  99.0\n",
      "Mean Reward  39.243027888446214\n",
      "Max reward so far:  217.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  251\n",
      "Reward:  146.0\n",
      "Mean Reward  39.666666666666664\n",
      "Max reward so far:  217.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  252\n",
      "Reward:  144.0\n",
      "Mean Reward  40.07905138339921\n",
      "Max reward so far:  217.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  253\n",
      "Reward:  188.0\n",
      "Mean Reward  40.661417322834644\n",
      "Max reward so far:  217.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  254\n",
      "Reward:  96.0\n",
      "Mean Reward  40.87843137254902\n",
      "Max reward so far:  217.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  255\n",
      "Reward:  97.0\n",
      "Mean Reward  41.09765625\n",
      "Max reward so far:  217.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  256\n",
      "Reward:  55.0\n",
      "Mean Reward  41.15175097276265\n",
      "Max reward so far:  217.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  257\n",
      "Reward:  37.0\n",
      "Mean Reward  41.13565891472868\n",
      "Max reward so far:  217.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  258\n",
      "Reward:  69.0\n",
      "Mean Reward  41.24324324324324\n",
      "Max reward so far:  217.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  259\n",
      "Reward:  136.0\n",
      "Mean Reward  41.60769230769231\n",
      "Max reward so far:  217.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  260\n",
      "Reward:  67.0\n",
      "Mean Reward  41.70498084291188\n",
      "Max reward so far:  217.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  261\n",
      "Reward:  99.0\n",
      "Mean Reward  41.9236641221374\n",
      "Max reward so far:  217.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  262\n",
      "Reward:  101.0\n",
      "Mean Reward  42.148288973384034\n",
      "Max reward so far:  217.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  263\n",
      "Reward:  115.0\n",
      "Mean Reward  42.42424242424242\n",
      "Max reward so far:  217.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  264\n",
      "Reward:  180.0\n",
      "Mean Reward  42.943396226415096\n",
      "Max reward so far:  217.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  265\n",
      "Reward:  231.0\n",
      "Mean Reward  43.650375939849624\n",
      "Max reward so far:  231.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  266\n",
      "Reward:  185.0\n",
      "Mean Reward  44.17977528089887\n",
      "Max reward so far:  231.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  267\n",
      "Reward:  32.0\n",
      "Mean Reward  44.134328358208954\n",
      "Max reward so far:  231.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  268\n",
      "Reward:  129.0\n",
      "Mean Reward  44.44981412639405\n",
      "Max reward so far:  231.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  269\n",
      "Reward:  196.0\n",
      "Mean Reward  45.01111111111111\n",
      "Max reward so far:  231.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  270\n",
      "Reward:  244.0\n",
      "Mean Reward  45.745387453874535\n",
      "Max reward so far:  244.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  271\n",
      "Reward:  68.0\n",
      "Mean Reward  45.82720588235294\n",
      "Max reward so far:  244.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  272\n",
      "Reward:  363.0\n",
      "Mean Reward  46.989010989010985\n",
      "Max reward so far:  363.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  273\n",
      "Reward:  287.0\n",
      "Mean Reward  47.86496350364963\n",
      "Max reward so far:  363.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  274\n",
      "Reward:  312.0\n",
      "Mean Reward  48.82545454545455\n",
      "Max reward so far:  363.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  275\n",
      "Reward:  324.0\n",
      "Mean Reward  49.822463768115945\n",
      "Max reward so far:  363.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  276\n",
      "Reward:  160.0\n",
      "Mean Reward  50.2202166064982\n",
      "Max reward so far:  363.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  277\n",
      "Reward:  82.0\n",
      "Mean Reward  50.33453237410072\n",
      "Max reward so far:  363.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  278\n",
      "Reward:  165.0\n",
      "Mean Reward  50.74551971326165\n",
      "Max reward so far:  363.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  279\n",
      "Reward:  337.0\n",
      "Mean Reward  51.767857142857146\n",
      "Max reward so far:  363.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  280\n",
      "Reward:  261.0\n",
      "Mean Reward  52.512455516014235\n",
      "Max reward so far:  363.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  281\n",
      "Reward:  299.0\n",
      "Mean Reward  53.38652482269504\n",
      "Max reward so far:  363.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  282\n",
      "Reward:  114.0\n",
      "Mean Reward  53.600706713780916\n",
      "Max reward so far:  363.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  283\n",
      "Reward:  142.0\n",
      "Mean Reward  53.91197183098591\n",
      "Max reward so far:  363.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  284\n",
      "Reward:  70.0\n",
      "Mean Reward  53.96842105263158\n",
      "Max reward so far:  363.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  285\n",
      "Reward:  330.0\n",
      "Mean Reward  54.93356643356643\n",
      "Max reward so far:  363.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  286\n",
      "Reward:  320.0\n",
      "Mean Reward  55.857142857142854\n",
      "Max reward so far:  363.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  287\n",
      "Reward:  143.0\n",
      "Mean Reward  56.15972222222222\n",
      "Max reward so far:  363.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  288\n",
      "Reward:  35.0\n",
      "Mean Reward  56.08650519031142\n",
      "Max reward so far:  363.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  289\n",
      "Reward:  229.0\n",
      "Mean Reward  56.682758620689654\n",
      "Max reward so far:  363.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  290\n",
      "Reward:  89.0\n",
      "Mean Reward  56.79381443298969\n",
      "Max reward so far:  363.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  291\n",
      "Reward:  216.0\n",
      "Mean Reward  57.33904109589041\n",
      "Max reward so far:  363.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  292\n",
      "Reward:  330.0\n",
      "Mean Reward  58.26962457337884\n",
      "Max reward so far:  363.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  293\n",
      "Reward:  189.0\n",
      "Mean Reward  58.714285714285715\n",
      "Max reward so far:  363.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  294\n",
      "Reward:  36.0\n",
      "Mean Reward  58.637288135593224\n",
      "Max reward so far:  363.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  295\n",
      "Reward:  480.0\n",
      "Mean Reward  60.060810810810814\n",
      "Max reward so far:  480.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  296\n",
      "Reward:  383.0\n",
      "Mean Reward  61.148148148148145\n",
      "Max reward so far:  480.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  297\n",
      "Reward:  332.0\n",
      "Mean Reward  62.057046979865774\n",
      "Max reward so far:  480.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  298\n",
      "Reward:  232.0\n",
      "Mean Reward  62.625418060200666\n",
      "Max reward so far:  480.0\n",
      "Model Saved\n",
      "=======================================\n",
      "Episode:  299\n",
      "Reward:  697.0\n",
      "Mean Reward  64.74\n",
      "Max reward so far:  697.0\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "allRewards = []\n",
    "total_rewards = 0\n",
    "maximumRewardRecorded = 0\n",
    "episode = 0\n",
    "episode_states, episode_actions, episode_rewards = [], [], []\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "#tf.reset_default_graph()\n",
    "with tf.Session() as sess :\n",
    "    \n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for episode in range(max_episodes) :\n",
    "        \n",
    "        episode_rewards_sum = 0\n",
    "        \n",
    "        # Launch the game\n",
    "        state = env.reset()\n",
    "        \n",
    "        while True :\n",
    "            \n",
    "            # Choose action a, remember WE'RE NOT IN A DETERMINISTIC ENVIRONMENT, WE'RE OUTPUT PROBABILITIES\n",
    "            action_probability_distribution = sess.run(action_distribution,feed_dict={input_:state.reshape([1,4])})\n",
    "            action = np.random.choice(range(action_probability_distribution.shape[1]),p=action_probability_distribution.ravel()) # Select action w.r.t the actions prob\n",
    "            \n",
    "            # Perform a\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Store s, a, r\n",
    "            episode_states.append(state)\n",
    "            \n",
    "            # For actions because we output only one (the index) we need 2 (1 is for the action taken)\n",
    "            # We need [0., 1.] (if we take right) not just the index\n",
    "            action_ = np.zeros(action_size)\n",
    "            action_[action] = 1\n",
    "            \n",
    "            episode_actions.append(action_)\n",
    "            \n",
    "            episode_rewards.append(reward)\n",
    "            \n",
    "            if done :\n",
    "                # Calculate sum reward\n",
    "                episode_rewards_sum = np.sum(episode_rewards)\n",
    "                \n",
    "                allRewards.append(episode_rewards_sum)\n",
    "                \n",
    "                total_rewards = np.sum(allRewards)\n",
    "                \n",
    "                # Mean reward\n",
    "                mean_reward = np.divide(total_rewards,episode + 1)\n",
    "                \n",
    "                maximumRewardRecorded = np.amax(allRewards)\n",
    "                \n",
    "                print(\"=======================================\")\n",
    "                print(\"Episode: \",episode)\n",
    "                print(\"Reward: \",episode_rewards_sum)\n",
    "                print(\"Mean Reward \",mean_reward)\n",
    "                print(\"Max reward so far: \",maximumRewardRecorded)\n",
    "                \n",
    "                # Calculate discounted reward\n",
    "                discounted_episode_rewards = discount_and_normalize_rewards(episode_rewards)\n",
    "                \n",
    "                # Feedforward, gradient and backpropogation\n",
    "                loss_, _ = sess.run([loss,train_opt],feed_dict={input_:np.vstack(np.array(episode_states)),\n",
    "                                                                actions:np.vstack(np.array(episode_actions)),\n",
    "                                                                discounted_episode_rewards_:discounted_episode_rewards})\n",
    "                \n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op,feed_dict={input_:np.vstack(np.array(episode_states)),\n",
    "                                                       actions:np.vstack(np.array(episode_actions)),\n",
    "                                                       discounted_episode_rewards_:discounted_episode_rewards,\n",
    "                                                       mean_reward_:mean_reward})\n",
    "                \n",
    "                writer.add_summary(summary,episode)\n",
    "                writer.flush()\n",
    "                \n",
    "                # Reset transition scores\n",
    "                episode_states, episode_actions, episode_rewards = [], [], []\n",
    "                \n",
    "                break\n",
    "                \n",
    "            state = new_state\n",
    "            \n",
    "        # Save Model\n",
    "        if episode % 100 :\n",
    "            saver.save(sess,\"./models/model.ckpt\")\n",
    "            print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "**************************************\n",
      "EPISODE  0\n",
      "Score 263.0\n",
      "Score over time: 26.3\n",
      "**************************************\n",
      "EPISODE  1\n",
      "Score 603.0\n",
      "Score over time: 86.6\n",
      "**************************************\n",
      "EPISODE  2\n",
      "Score 341.0\n",
      "Score over time: 120.7\n",
      "**************************************\n",
      "EPISODE  3\n",
      "Score 289.0\n",
      "Score over time: 149.6\n",
      "**************************************\n",
      "EPISODE  4\n",
      "Score 410.0\n",
      "Score over time: 190.6\n",
      "**************************************\n",
      "EPISODE  5\n",
      "Score 20.0\n",
      "Score over time: 192.6\n",
      "**************************************\n",
      "EPISODE  6\n",
      "Score 470.0\n",
      "Score over time: 239.6\n",
      "**************************************\n",
      "EPISODE  7\n",
      "Score 239.0\n",
      "Score over time: 263.5\n",
      "**************************************\n",
      "EPISODE  8\n",
      "Score 286.0\n",
      "Score over time: 292.1\n",
      "**************************************\n",
      "EPISODE  9\n",
      "Score 342.0\n",
      "Score over time: 326.3\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess :\n",
    "    env.reset()\n",
    "    rewards = []\n",
    "    \n",
    "    # Load the model\n",
    "    saver.restore(sess,\"./models/model.ckpt\")\n",
    "    \n",
    "    for episode in range(10) :\n",
    "        state = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards = 0\n",
    "        print(\"**************************************\")\n",
    "        print(\"EPISODE \",episode)\n",
    "        \n",
    "        while True :\n",
    "            \n",
    "            # Chose action a, remeber WE'RE NOT IN A DETERMINISTIC ENVIRONMENT, WE'RE OUPUT PROBABILITIES\n",
    "            action_probability_distribution = sess.run(action_distribution,feed_dict={input_:state.reshape([1,4])})\n",
    "            \n",
    "            # print (action_probability_distribution)\n",
    "            action = np.random.choice(range(action_probability_distribution.shape[1]),p=action_probability_distribution.ravel()) # select action w.r.t the action prob\n",
    "            \n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            total_rewards += reward\n",
    "            \n",
    "            if done :\n",
    "                rewards.append(total_rewards)\n",
    "                print(\"Score\",total_rewards)\n",
    "                break\n",
    "                \n",
    "            state = new_state\n",
    "        env.close()\n",
    "        print(\"Score over time: \" + str(sum(rewards)/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
