{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sourabh\\AppData\\Local\\conda\\conda\\envs\\tensorflow_env\\lib\\site-packages\\IPython\\core\\display.py:689: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-Ynjw0Vl3i4?showinfo=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-Ynjw0Vl3i4?showinfo=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # Deep Learning Library\n",
    "import numpy as np # Handle Matrices\n",
    "from vizdoom import * # Doom Environment\n",
    "\n",
    "import random   # Handling random number generation\n",
    "import time     # Handling time calculation\n",
    "from skimage import transform  # Help us preprocess the frames\n",
    "\n",
    "from collections import deque  # Ordered collection with ends\n",
    "import matplotlib.pyplot as plt # Display graphs\n",
    "\n",
    "import warnings # This will ignore all the warning messages that are normally printed during training because of skimage\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create our enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we create our environment\n",
    "\"\"\"\n",
    "\n",
    "def create_environment() :\n",
    "    \n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration\n",
    "    game.load_config(\"deadly_corridor.cfg\")\n",
    "    \n",
    "    # Load the correct scenario (in our case deadly_corridor scenario)\n",
    "    game.set_doom_scenario_path(\"deadly_corridor.wad\")\n",
    "    \n",
    "    game.init()\n",
    "    \n",
    "    # Here we create an hot encoded version of our actions (5 possible actions)\n",
    "    # possible_actions = [[1, 0, 0, 0, 0],[0, 1, 0, 0, 0]...]\n",
    "    possible_actions = np.identity(7,dtype=int).tolist()\n",
    "    \n",
    "    return game, possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, possible_actions = create_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame) :\n",
    "    \n",
    "    # Crop the screen (remove the part that contains no information)\n",
    "    # [Up: Down, Left: right]\n",
    "    cropped_frame = frame[15,:-5,20:-20]\n",
    "    \n",
    "    # Normalize Pixel Values\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "    \n",
    "    # Resize\n",
    "    preprocessed_frame = transform.resize(cropped_frame,[100,120])\n",
    "    \n",
    "    return preprocessed_frame # 100x120x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define stack_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4 # We stack 4 frames\n",
    "\n",
    "# Initialize deque with zero-images one array for each image\n",
    "stacked_frames = deque([np.zeros((100,120),dtype=np.int) for i in range(stack_size)],maxlen=4)\n",
    "\n",
    "def stack_frames(stacked_frames,state,is_new_episode) :\n",
    "    # Preprocess frame\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode :\n",
    "        # Clear our stacked_frames\n",
    "        stacked_frames = deque([np.zeros((100,120),dtype=np.int) for i in range(stack_size)],maxlen=4)\n",
    "        \n",
    "        # Because we're in a new episode, copy the frame 4x\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames,axis=2)\n",
    "    else :\n",
    "        # Append the frame to deque, automatically removes the oldest frame\n",
    "        stacked_frame.append(frame)\n",
    "        \n",
    "        # Build the stacked state (first dimension specified different frames)\n",
    "        stacked_state = np.stack(stacked_frames,axis=2)\n",
    "        \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 : Setting up our hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Hyperparameters\n",
    "state_size = [100,120,4] # Our input is a stack of 4 frames hence 100x120x4 (Width, height, channels)\n",
    "action_size = game.get_available_buttons_size() # 7 possible actions \n",
    "learning_rate = 0.00025 # Alpha (aka learning rate)\n",
    "\n",
    "# Training Hyperparameters\n",
    "total_episodes = 500 # Total episodes for training\n",
    "max_steps = 500 # Max possible steps in an episode\n",
    "batch_size = 64\n",
    "\n",
    "# FIXED Q Targets Hyperparameters\n",
    "max_tau = 1000 # Tau is the C step where we update our target network\n",
    "\n",
    "# Exploration Hyperparameters\n",
    "explore_start = 1.0 # exploration probability at start\n",
    "explore_stop = 0.01 # minimum exploration probability\n",
    "decay_rate = 0.0005 # exponential decay rate for exploration\n",
    "\n",
    "# Q Learning hyperparameters\n",
    "gamma = 0.95  # discounting rate\n",
    "\n",
    "## Memory Hyperparameters\n",
    "pretrain_length = 1000 # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 1000 # Number of experiences the Memory can keep\n",
    "\n",
    "# Modify this to False if you just want to see the trained agent\n",
    "training = True\n",
    "# Turn this to True, if you want to render the environment\n",
    "episode_render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDDQNNet :\n",
    "    def __init__(self,state_size,action_size,learning_rate,name) :\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.name = name\n",
    "        \n",
    "        # We use tf.variable_scope here to know which network we're using (DQN or target_network)\n",
    "        # it will be useful whrn we will update our w- parameters (by copy the DQN parameters)\n",
    "        with tf.variable_scope(self.name) :\n",
    "            \n",
    "            # We create the placeholders\n",
    "            # *state_size means that we take each elements of state_size in tuple hence it's like we wrote [None,100,120,4]\n",
    "            self.inputs_ = tf.placeholder(tf.float32,[None,*state_size],name=\"inputs\")\n",
    "            \n",
    "            self.ISWeights_ = tf.placeholder(tf.float32,[None,1],name=\"IS_weights\")\n",
    "            \n",
    "            self.actions_ = tf.placeholder(tf.float32,[None,action_size],name=\"actions_\")\n",
    "            \n",
    "            # Remember that target_Q is the R(s,a) + y*max_a Q_hat(s',a')\n",
    "            self.target_Q = tf.placeholder(tf.float32,[None],name=\"target\")\n",
    "            \n",
    "            \"\"\"\n",
    "            First convnet :\n",
    "            CNN\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            # Input is 100x120x4\n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.inputs_,\n",
    "                                          filters = 32,\n",
    "                                          kernel_size = [8,8],\n",
    "                                          strides = [4,4],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name=\"conv1\")\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1,name=\"conv1_out\")\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            Second convnet :\n",
    "            CNN\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            \n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.conv1_out,\n",
    "                                          filters=64,\n",
    "                                          kernel_size=[4,4],\n",
    "                                          strides=[2,2],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name=\"conv2\")\n",
    "            \n",
    "            self.conv2_out = tf.nn.elu(self.conv2,name=\"conv2_out\")\n",
    "            \n",
    "            \"\"\"\n",
    "            Third convnet :\n",
    "            CNN\n",
    "            ELU\n",
    "            \"\"\"\n",
    "            \n",
    "            self.conv3 = tf.layers.conv2d(inputs=self.conv2_out,\n",
    "                                          filters=128,\n",
    "                                          kernel_size=[4,4],\n",
    "                                          strides=[2,2],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name=\"conv3\")\n",
    "            \n",
    "            self.conv3_out = tf.nn.elu(self.conv3,name=\"conv3_out\")\n",
    "            \n",
    "            self.flatten = tf.layers.flatten(self.conv3_out)\n",
    "            \n",
    "            # Here we separate into two streams \n",
    "            # The one that calculate V(s)\n",
    "            self.value_fc = tf.layers.dense(inputs=self.flatten,\n",
    "                                            units=512,\n",
    "                                            activation=tf.nn.elu,\n",
    "                                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                            name=\"value_fc\")\n",
    "            \n",
    "            self.value = tf.layers.dense(inputs=self.value_fc,\n",
    "                                         units=1,\n",
    "                                         activation=None,\n",
    "                                         kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                         name=\"value\")\n",
    "            \n",
    "            # The one that calcuates A(s,a)\n",
    "            self.advantage_fc = tf.layers.dense(inputs=self.flatten,\n",
    "                                                units=512,\n",
    "                                                activation=None,\n",
    "                                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                                name=\"advantages_fc\")\n",
    "            \n",
    "            self.advantage = tf.layers.dense(inputs=self.advantage_fc,\n",
    "                                             units=self.action_size,\n",
    "                                             activation=None,\n",
    "                                             kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                             name=\"advantages\")\n",
    "            \n",
    "            # Aggregation Layer\n",
    "            # Q(s,a) = V(s,a) + (A(s,a) 1/|A|*sum A(s,a'))\n",
    "            \n",
    "            self.output = self.value + tf.subtract(self.advantage,tf.reduce_mean(self.advantage,axis=1,keepdims=True))\n",
    "            \n",
    "            # Q is our predicted Q value\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output,self.actions_),axis=1)\n",
    "            \n",
    "            # The loss is modifier because of PER\n",
    "            self.absoulute_errors = tf.abs(self.target_Q - self.Q) # for updating the sum tree\n",
    "            \n",
    "            self.loss = tf.reduce_mean(self.ISWeights_*tf.squared_difference(self.target_Q,self.Q))\n",
    "            \n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Instantitate the DQNetwork\n",
    "DQNetwork = DDDQNNet(state_size,action_size,learning_rate,name=\"DQNetwork\")\n",
    "\n",
    "# Instantiate the target network\n",
    "TargetNetwork = DDDQNNet(state_size,action_size,learning_rate,name=\"TargetNetwork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree(object) :\n",
    "    \"\"\"\n",
    "    This SumTree code is modified version of Morvan Zhou\n",
    "    \"\"\"\n",
    "    \n",
    "    data_pointer = 0\n",
    "    \n",
    "    \"\"\"\n",
    "    Here we initialize the tree with all nodes = 0, and initialize the data with all values = 0\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,capacity) :\n",
    "        self.capacity = capacity # Number of leaf nodes (final nodes) that contain experiences\n",
    "        \n",
    "        # Generate the tree with all nodes values = 0\n",
    "        # To understand this calculation (2*capacity - 1) look at the schema above\n",
    "        # Rember we are in a binary node (each node has max 2 children) so 2x size of leaf (caapacity) - 1 (root node)\n",
    "        # Parent nodes = capacity - 1\n",
    "        # Leaf nodes = capacity\n",
    "        \n",
    "        self.tree = np.zeros(2*capacity - 1)\n",
    "        \n",
    "        # contains the experiences (so the size of data is the capacity)\n",
    "        self.data = np.zeros(capacity,dtype=object)\n",
    "        \n",
    "    \"\"\"\n",
    "    Here we add our proiority score in the sumtree leaf and add the experience in data\n",
    "    \"\"\"\n",
    "    def add(self,priority,data) :\n",
    "        # Look at what index we want to put the experience\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "        \n",
    "        # Update the data frame\n",
    "        self.data[self.data_pointer] = data\n",
    "        \n",
    "        # Add 1 to the data_pointer\n",
    "        self.data_pointer += 1\n",
    "        \n",
    "        if self.data_pointer >= self.capacity : # If we're above the capacity, you go back to first index (we overwrite)\n",
    "            self.data_pointer = 0\n",
    "            \n",
    "    \"\"\"\n",
    "    Update the leaf priority score and propogate the change through the tree\n",
    "    \"\"\"\n",
    "    def update(self,tree_index,priority) :\n",
    "        # Change = new priority score - former priority score\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "        \n",
    "        # then propogate the change through the tree\n",
    "        while tree_index  != 0 : # this method ios faster than the recursive loop in the reference code\n",
    "            \n",
    "            \"\"\"\n",
    "            Here we want to access the line above \n",
    "            THE NUMBERS IN THIS TREE ARE THE INDEXES NOT THE PRIORITY VALUES\n",
    "            \"\"\"\n",
    "            \n",
    "            tree_index = (tree_index - 1)//2\n",
    "            self.tree[tree_index] += change\n",
    "            \n",
    "    \"\"\"\n",
    "    Here we get the leaf_index, priority value of that leaf and experience associated with that index\n",
    "    \"\"\"\n",
    "    def get_leaf(self,v) :\n",
    "        \n",
    "        parent_index = 0\n",
    "        while True :\n",
    "            left_child_index = 2*parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "            \n",
    "            # If we reach the bottom, end the search\n",
    "            if left_child_index >= len(self.tree) :\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            else : # downward search, always search for a higher priority node\n",
    "                if v <= self.tree[left_child_index] :\n",
    "                    parent_index = left_child_index\n",
    "                else :\n",
    "                    parent_index = right_child_index\n",
    "                    \n",
    "        data_index = leaf_index + self.capacity + 1\n",
    "        \n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "    \n",
    "    @property\n",
    "    def total_priority(self) :\n",
    "        return self.tree[0] # Returns the root node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we don't use deque anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object) : # stored as (s,a,r,s_) in SumTree\n",
    "    \n",
    "    PER_e = 0.01 # Hyperparameters that we use to avoid some experiences to have 0 probability of being taken\n",
    "    PER_a = 0.6 # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "    \n",
    "    PER_b = 0.4 # importance-sampling, from initial value increasing to 1\n",
    "    PER_b_increment_per_sampling = 0.001\n",
    "    absolute_error_upper = 1 # clipped abs error\n",
    "    \n",
    "    def __init__(self,capacity) :\n",
    "        # Making the tree\n",
    "        \n",
    "        \"\"\"\n",
    "        Remember that our tree is composed of a sum tree that contains the priority scores at his leaf\n",
    "        And also a data array\n",
    "        We don't use deque because it means that at each timestep our experiences change index by one.\n",
    "        We prefer to use a simple array and to overwrite when the memory is full\n",
    "        \"\"\"\n",
    "        \n",
    "        self.tree = SumTree(capacity)\n",
    "        \n",
    "    \"\"\"\n",
    "    Store a new experience in our tree\n",
    "    Each new experience have a score of max_priority (it will be then improved when we use this exp to train our DQN)\n",
    "    \"\"\"\n",
    "    \n",
    "    def store(self,experience) :\n",
    "        # Find the max priority\n",
    "        max_priority = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        \n",
    "        # If the max priority  = 0 we can't put priority = 0 since this exp will never have a chanve to be selected\n",
    "        # So we use a minimum priority\n",
    "        if max_priority == 0 :\n",
    "            max_priority = self.absoulute_error_upper\n",
    "            \n",
    "        self.tree.add(max_priority,experience) # set the max p for new p\n",
    "        \n",
    "    \"\"\"\n",
    "    - First, to sample a minibatch of k size, the range [0,priority_total] is / into k ranges.\n",
    "    - Then a value is uniformly sampled from each range\n",
    "    - We search in the sumtree, the experience where priority score correspond to sample values are retreived from.\n",
    "    - Then, we calculate IS weights for each minibatch element\n",
    "    \"\"\"\n",
    "    \n",
    "    def sample(self,n) :\n",
    "        \n",
    "        # Create a sample array that will contain the minibatch\n",
    "        memory_b = []\n",
    "        \n",
    "        b_idx, b_ISWeights = np.empty((n,),dtype=np.int32), np.empty((n,1),dtype=np.float32)\n",
    "        \n",
    "        # Calculate the priority segment\n",
    "        # Here as we explained in the paper we divide the Range[0,ptotal] into n range\n",
    "        priority_segment = self.tree.total_priority / n    # priority segment\n",
    "        \n",
    "        # Here we increasing the PER_b each time we sample a new minibatch\n",
    "        self.PER_b = np.min([1.,self.PER_b + self.PER_b_increment_per_sampling])  # max = 1\n",
    "        \n",
    "        # Calculating the max_weight\n",
    "        p_min = np.min(self.tree.tree[-self.tree.tree[-self.tree.capacity:]]) / self.tree.total_priority\n",
    "        max_weight = (p_min*n)**(-self.PER_b)\n",
    "        \n",
    "        for i in range(n) :\n",
    "            \"\"\"\n",
    "            A value is uniformly sample from each range\n",
    "            \"\"\"\n",
    "            \n",
    "            a, b = priority_segment * i, priority_segment * (i + 1)\n",
    "            value = np.random.uniform(a,b)\n",
    "            \n",
    "            \"\"\"\n",
    "            Experience that correpond to each value is retreived\n",
    "            \"\"\"\n",
    "            index, priority, data = self.tree.get_leaf(value)\n",
    "            \n",
    "            # P(j)\n",
    "            sampling_probabilities = priority / self.treee.total_priority\n",
    "            \n",
    "            # IS = (1/N * 1 / P(i))**b / max wi == (N*P(i))**-b / max wi\n",
    "            b_ISWeights[i,0] = np.power(n*sampling_probabilities,-self.PER_b) / max_weight\n",
    "            \n",
    "            b_idx[i] = index\n",
    "            \n",
    "            experience = [data]\n",
    "            \n",
    "            memory_b.append(experience)\n",
    "            \n",
    "        return b_idx, memory_b, b_ISWeights\n",
    "    \n",
    "    \"\"\"\n",
    "    Update the priorities on the tree\n",
    "    \"\"\"\n",
    "    \n",
    "    def batch_update(self,tree_idx,abs_errors) :\n",
    "        \n",
    "        abs_errors += self.PER_e  # convert to abs and avoid 0\n",
    "        clipped_errors = np.minimum(abs_errors,self.absolute_error_upper)\n",
    "        ps = np.power(clipped_errors,self.PER_a)\n",
    "        \n",
    "        for ti, p in zip(tree_idx,ps) :\n",
    "            self.tree.update(ti,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll deal with the empty memory problem, we pre-populate our memory by taking random actions and stoeing the experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate memory\n",
    "memory = Memory(memory_size)\n",
    "\n",
    "# Render the environment\n",
    "game.new_episode()\n",
    "\n",
    "for i in range(pretrain_length) :\n",
    "    # If it's the first step\n",
    "    if i == 0 :\n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames,state,True)\n",
    "        \n",
    "    # Random action\n",
    "    action = random.choice(possible_actions)\n",
    "    \n",
    "    # Get the rewards\n",
    "    reward = game.make_action(action)\n",
    "    \n",
    "    # Look if the episode is finished\n",
    "    done = game.is_episode_finished()\n",
    "    \n",
    "    # If we're dead\n",
    "    if done :\n",
    "        # We finished the episode\n",
    "        next_state  = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        # experience = np.hstack((state,[action,reward],next_state,done))\n",
    "        \n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "        \n",
    "        # Start a new episode\n",
    "        game.new_episode()\n",
    "        \n",
    "        # First we need a state\n",
    "        state = game.get_state().screen_buffer\n",
    "        \n",
    "        # Stack the frames\n",
    "        state, stacked_frames = stack_frames(stacked_frames,state,True)\n",
    "        \n",
    "    else :\n",
    "        # Get the next state\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames,next_state,False)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        experience = state, action, reward, next_state, done\n",
    "        memory.store(experience)\n",
    "        \n",
    "        # Our state is now the next state\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/dddqn/1\")\n",
    "\n",
    "# Losses\n",
    "tf.summary.scalar(\"Loss\",DQNetwork.loss)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will do the part\n",
    "with epsilon select a random action a(t), otherwise select a(t) = argmax_a Q(s(t),a)\n",
    "\"\"\"\n",
    "\n",
    "def predict_action(explore_start,explore_stop,decay_rate,decay_step,state,actions) :\n",
    "    # Epsilon Greedy Strategy\n",
    "    # Choose action a from state s using epsilon greedy.\n",
    "    # First we randomize a number\n",
    "    exp_exp_tradeoff = np.radnom.rand()\n",
    "    \n",
    "    # Here we'll use an   improved version of our epsilon greedy strategy used in Q-Learning\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*decay_step)\n",
    "    \n",
    "    if (explore_probability > exp_exp_tradeoff) :\n",
    "        # Make a random action (exploration)\n",
    "        action = random.choice(possible_actions)\n",
    "        \n",
    "    else :\n",
    "        # Get action from Q-network (exploitation)\n",
    "        # Estimate the Qs values state\n",
    "        \n",
    "        Qs = sess.run(DQNetwork.output,feed_dict={DQNetwork.inputs_:state.reshape((1,*state.shape))})\n",
    "        \n",
    "        # Take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "        \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function helps us to copy one set of varibles to another\n",
    "# In our case we use it when we want to copy the parameters  of DQN to Target_network\n",
    "\n",
    "def update_target_graph() :\n",
    "    \n",
    "    # Get the parameters of our DQNNetwork\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"DQNetwork\")\n",
    "    \n",
    "    # Get the parameters of our Target_network\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\"TargetNetwork\")\n",
    "    \n",
    "    op_holder = []\n",
    "    \n",
    "    # Update our target_network parameters with DQNNetwork parameters\n",
    "    for from_var, to_var in zip(from_vars,to_vars) :\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saver will help us to save our model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training == True :\n",
    "    with tf.Session() as sess :\n",
    "        # Initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Initialize the decay rate (that will use to reduce epsilon)\n",
    "        decay_step = 0\n",
    "        \n",
    "        # Set tau = 0\n",
    "        tau = 0\n",
    "        \n",
    "        # Init the game\n",
    "        game.init()\n",
    "        \n",
    "        # Update the parameters of our TargetNetwork with DQN_weights\n",
    "        update_target = update_target_graph()\n",
    "        sess.run(update_target)\n",
    "        \n",
    "        for episode in range(total_episode) :\n",
    "            # Set step to 0\n",
    "            step = 0\n",
    "            \n",
    "            # Initialize the rewards of the episode\n",
    "            episode_rewards = []\n",
    "            \n",
    "            # Make a new episode and observe the first state\n",
    "            game.new_episode()\n",
    "            \n",
    "            state = game.get_state().screen_buffer\n",
    "            \n",
    "            # Remeber that stack frame function also call our preprocess function\n",
    "            state, stacked_frames = stack_frames(stacked_frames,state,True)\n",
    "            \n",
    "            while step < max_steps :\n",
    "                step += 1\n",
    "                \n",
    "                # Increase the C step\n",
    "                tau += 1\n",
    "                \n",
    "                # Increae the decay_step\n",
    "                decay_step += 1\n",
    "                \n",
    "                # With e select a random action a(t), otherwise select a(t) = argmax_a Q(s(t),a)\n",
    "                action, explore_probability = predict_action(explore_start,explore_stop,decay_rate,decay_step,state,possible_actions)\n",
    "                \n",
    "                # Do the action\n",
    "                reward = game.make_action(action)\n",
    "                \n",
    "                # Look if the episode is finished\n",
    "                done = game.is_episode_finished()\n",
    "                \n",
    "                # Add the reward to the total reward\n",
    "                episode_rewards.append(reward)\n",
    "                \n",
    "                # If the game is finished\n",
    "                if done :\n",
    "                    # the episode ends so no next state\n",
    "                    next_state = np.zeros((120,140),dtype=np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames,next_state,False)\n",
    "                    \n",
    "                    # Set step = max_steps to end the episode\n",
    "                    step = max_steps\n",
    "                    \n",
    "                    # Get the total reward of the episode\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "                    \n",
    "                    print('Episode: {}'.format(episode),\n",
    "                          'Total reward: {}'.format(total_reward),\n",
    "                          'Training Loss: {:.4f}'.format(loss),\n",
    "                          'Explore P: {:.4f}'.format(explore_probability))\n",
    "                    \n",
    "                    # Add experience to memory\n",
    "                    experience = state, action, reward, next_state, done\n",
    "                    memory.store(experience)\n",
    "                    \n",
    "                else :\n",
    "                    # Get the next state\n",
    "                    next_state = game.get_state().screen_buffer\n",
    "                    \n",
    "                    # Stack the frame of the next_state\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames,next_state,False)\n",
    "                    \n",
    "                    # Add experience to memory\n",
    "                    experience = state, action, reward, next_state, done\n",
    "                    memory.store(experience)\n",
    "                    \n",
    "                    # s(t+1) is now our current state\n",
    "                    state = next_state\n",
    "                    \n",
    "                ## LEARNING PART\n",
    "                # Obtain random mini-batch from memory\n",
    "                tree_idx, batch, ISWeights_mb = memory.sample(batch_size)\n",
    "                \n",
    "                state_mb = np.array([each[0][0] for each in batch],ndmin=3)\n",
    "                actions_mb = np.array([each[0][1] for each in batch])\n",
    "                rewards_mb = np.array([each[0][2] for each in batch])\n",
    "                next_states_mb = np.array([each[0][3] for each in batch])\n",
    "                dones_mb = np.array([each[0][4] for each in batch])\n",
    "                \n",
    "                target_Qs_batch = []\n",
    "                \n",
    "                ### DOUBLE DQN Logic\n",
    "                # Use DQNNetwork to select the action to take at next_state (a') (action with the highest Q-value)\n",
    "                # Use TargetNetwork to calculate the Q_vaL of Q(s',a')\n",
    "                q_next_state = sess.run(DQNetwork.output, feed_dict={DQNetwork.inputs_:next_states_mb})\n",
    "                \n",
    "                # Get Q values for next_state\n",
    "                q_target_next_state = sess.run(TargetNetwork.output,feed_dict={TargetNetwork.inputs_:next_states_mb})\n",
    "                \n",
    "                # Set Q_target = r if the episide ends at s+1, otherwise set Q_target = r + gamma*Q_target(s',a')\n",
    "                \n",
    "                for i in range(0,len(batch)) :\n",
    "                    terminal = dones_mb[i]\n",
    "                    \n",
    "                    # We got a'\n",
    "                    action = np.argmax(q_next_state[i])\n",
    "                    \n",
    "                    # If we are in a terminal state, only equals reward\n",
    "                    if terminal :\n",
    "                        target_Qs_batch.append(rerwards_mb[i])\n",
    "                    else :\n",
    "                        # Take the Q_target for action a'\n",
    "                        target = rewards_mb[i] + gamma*q_target_next_state[i][action]\n",
    "                        target_Qs_batch.append(target)\n",
    "                    \n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "                \n",
    "                _, loss, absolute_errors = sess.run([DQNetwork.optimizer,DQNetwork.loss,DQNetwork.absolute_errors], feed_dict={DQNetwork.inputs_:states_mb,\n",
    "                                                                                                                               DQNetwork.target_Q:targets_mb,\n",
    "                                                                                                                               DQNetwork.actions_:actions_mb,\n",
    "                                                                                                                               DQNetwork.ISWeights_:ISWeights_mb})\n",
    "                \n",
    "                # Update priority\n",
    "                memory.batch_update(tree_idx,absolute_errors)\n",
    "                \n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op,feed_dict={DQNetwork.inputs_:states_mb,\n",
    "                                                       DQNetwork.target_Q:targets_mb,\n",
    "                                                       DQNetwork.actions_:actions_mb,\n",
    "                                                       DQNetwork.ISWeights_:ISweights_mb})\n",
    "                \n",
    "                writer.add_summary(summary,episode)\n",
    "                writer.flush()\n",
    "                \n",
    "                if tau > max_tau :\n",
    "                    # Update the parameters of our TargetNetwork with DQN_weights\n",
    "                    update_target = update_target_graph()\n",
    "                    sess.run(update_target)\n",
    "                    tau = 0\n",
    "                    print(\"Model updated\")\n",
    "                    \n",
    "                \n",
    "            # Save model every 5 episodes\n",
    "            if episode % 5 == 0 :\n",
    "                save_path = saver.save(sess,\"./models/model.ckpt\")\n",
    "                print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watching the agent play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess :\n",
    "    game = DoomGame()\n",
    "    \n",
    "    # Load the correct configuration (TESTING)\n",
    "    game.load_config(\"deadly_corridor_testing.cfg\")\n",
    "    \n",
    "    # Load the correct scenario (in our case deadly_corridor scenario)\n",
    "    game.set_doom_scenario_path(\"deadly_corridor.wad\")\n",
    "    \n",
    "    game.init()\n",
    "    \n",
    "    # Load the model\n",
    "    saver.restore(sess,\"./models/model.ckpt\")\n",
    "    game.init()\n",
    "    \n",
    "    for i in range(10) L:\n",
    "        game.new_episode()\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames,state,True)\n",
    "        \n",
    "        while not game.is_episode_finished() :\n",
    "            ## EPSILON GREEDY STRATEGY\n",
    "            # Choose action a from state s using epsilon greedy\n",
    "            # First we randomize a number\n",
    "            exp_exp_tradeoff = np.random.rand()\n",
    "            \n",
    "            explore_probability = 0.01\n",
    "            \n",
    "            if (explore_probability > exp_exp_tradeoff) :\n",
    "                # Make a random action (exploration)\n",
    "                action = random.choice(possible_actions)\n",
    "                \n",
    "            else :\n",
    "                # Get action from Q-network (exploitation)\n",
    "                # Estimate the Qs values state\n",
    "                Qs = sess.run(DQNetwork.output,feed_dict={DQNetwork.inputs_:state.reshape((1,*state.shape))})\n",
    "                \n",
    "                # Take the biggest Q value (= the best action)\n",
    "                choice = np.argmax(Qs)\n",
    "                action = possible_actions[int(choice)]\n",
    "                \n",
    "            game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "            \n",
    "            if done :\n",
    "                break\n",
    "            else :\n",
    "                next_state = game.get_state().screen_buffer\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames,next_state,False)\n",
    "                state = next_state\n",
    "            \n",
    "            score = game.get_total_reward()\n",
    "            print(\"Score: \",score)\n",
    "            \n",
    "        game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
